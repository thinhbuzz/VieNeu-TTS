import gradio as gr
print("‚è≥ ƒêang kh·ªüi ƒë·ªông VieNeu-TTS... Vui l√≤ng ch·ªù...")
import soundfile as sf
import tempfile
import shutil
import torch
from vieneu import VieNeuTTS, FastVieNeuTTS
import os
import sys
import time
import numpy as np
from typing import Generator, Optional, Tuple
import queue
import threading
import yaml
from vieneu_utils.core_utils import split_text_into_chunks, join_audio_chunks, env_bool
from functools import lru_cache
import gc

# --- CONSTANTS & CONFIG ---
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        _config = yaml.safe_load(f) or {}
except Exception as e:
    raise RuntimeError(f"Kh√¥ng th·ªÉ ƒë·ªçc config.yaml: {e}")

BACKBONE_CONFIGS = _config.get("backbone_configs", {})
CODEC_CONFIGS = _config.get("codec_configs", {})

_text_settings = _config.get("text_settings", {})
MAX_CHARS_PER_CHUNK = _text_settings.get("max_chars_per_chunk", 256)
MAX_TOTAL_CHARS_STREAMING = _text_settings.get("max_total_chars_streaming", 3000)

if not BACKBONE_CONFIGS or not CODEC_CONFIGS:
    raise ValueError("config.yaml thi·∫øu backbone_configs ho·∫∑c codec_configs")

# --- 1. MODEL CONFIGURATION ---
# Global model instance
tts = None
current_backbone = None
current_codec = None
model_loaded = False
using_lmdeploy = False

# Cache for reference texts
_ref_text_cache = {}

def get_available_devices() -> list[str]:
    """Get list of available devices for current platform."""
    devices = ["Auto", "CPU"]

    if sys.platform == "darwin":
        # macOS - check MPS
        if torch.backends.mps.is_available():
            devices.append("MPS")
    else:
        # Windows/Linux - check CUDA
        if torch.cuda.is_available():
            devices.append("CUDA")

    return devices

def get_model_status_message() -> str:
    """Reconstruct status message from global state"""
    global model_loaded, tts, using_lmdeploy, current_backbone, current_codec
    if not model_loaded or tts is None:
        return "‚è≥ Ch∆∞a t·∫£i model."
    
    backbone_config = BACKBONE_CONFIGS.get(current_backbone, {})
    codec_config = CODEC_CONFIGS.get(current_codec, {})
    
    backend_name = "üöÄ LMDeploy (Optimized)" if using_lmdeploy else "üì¶ Standard"
    
    # We don't track the exact device strings perfectly in global state, so we estimate
    device_info = "GPU" if using_lmdeploy else "Auto"
    codec_device = "CPU" if "ONNX" in (current_codec or "") else ("GPU/MPS" if torch.cuda.is_available() or torch.backends.mps.is_available() else "CPU")
    
    preencoded_note = "\n‚ö†Ô∏è Codec ONNX kh√¥ng h·ªó tr·ª£ ch·ª©c nƒÉng clone gi·ªçng n√≥i." if codec_config.get('use_preencoded') else ""
    
    opt_info = ""
    if using_lmdeploy and hasattr(tts, 'get_optimization_stats'):
        stats = tts.get_optimization_stats()
        opt_info = (
            f"\n\nüîß T·ªëi ∆∞u h√≥a:"
            f"\n  ‚Ä¢ Triton: {'‚úÖ' if stats['triton_enabled'] else '‚ùå'}"
            f"\n  ‚Ä¢ Max Batch Size (Default): {stats.get('max_batch_size', 'N/A')}"
            f"\n  ‚Ä¢ Reference Cache: {stats['cached_references']} voices"
            f"\n  ‚Ä¢ Prefix Caching: ‚úÖ"
        )

    return (
        f"‚úÖ Model ƒë√£ t·∫£i th√†nh c√¥ng!\n\n"
        f"üîß Backend: {backend_name}\n"
        f"ü¶ú Backbone: {current_backbone}\n"
        f"üéµ Codec: {current_codec}{preencoded_note}{opt_info}"
    )

def restore_ui_state():
    """Update UI components based on persistence"""
    global model_loaded
    msg = get_model_status_message()
    return (
        msg, 
        gr.update(interactive=model_loaded), # btn_generate
        gr.update(interactive=False)         # btn_stop
    )

def should_use_lmdeploy(backbone_choice: str, device_choice: str) -> bool:
    """Determine if we should use LMDeploy backend."""
    # LMDeploy not supported on macOS
    if sys.platform == "darwin":
        return False

    if "gguf" in backbone_choice.lower():
        return False

    if device_choice == "Auto":
        has_gpu = torch.cuda.is_available()
    elif device_choice == "CUDA":
        has_gpu = torch.cuda.is_available()
    else:
        has_gpu = False

    return has_gpu

@lru_cache(maxsize=32)
def get_ref_text_cached(text_path: str) -> str:
    """Cache reference text loading"""
    with open(text_path, "r", encoding="utf-8") as f:
        return f.read()

def cleanup_gpu_memory():
    """Aggressively cleanup GPU memory"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    elif torch.backends.mps.is_available():
        torch.mps.empty_cache()
    gc.collect()

def copy_output_if_configured(src_path: str, output_file_name: Optional[str] = None) -> Optional[str]:
    """Copy output wav to GRADIO_OUTPUT_DIR if configured."""
    output_dir = os.getenv("GRADIO_OUTPUT_DIR")
    if not output_dir:
        return None
    output_dir = os.path.expanduser(output_dir)
    os.makedirs(output_dir, exist_ok=True)

    if output_file_name and output_file_name.strip():
        raw_name = output_file_name.strip()
        if os.path.isabs(raw_name):
            raw_name = os.path.basename(raw_name)
        safe_rel = os.path.normpath(raw_name).lstrip("\\/")
        if safe_rel in (".", "") or safe_rel.startswith("..") or safe_rel.startswith("../") or safe_rel.startswith("..\\"):
            safe_rel = os.path.basename(src_path)
        name, ext = os.path.splitext(os.path.basename(safe_rel))
        if not ext:
            safe_rel = f"{safe_rel}.wav"
        elif ext.lower() != ".wav":
            safe_rel = os.path.join(os.path.dirname(safe_rel), f"{name}.wav")
        dest_path = os.path.join(output_dir, safe_rel)
    else:
        base_name = os.path.basename(src_path)
        dest_path = os.path.join(output_dir, base_name)

    if os.path.abspath(src_path) == os.path.abspath(dest_path):
        return dest_path

    dest_parent = os.path.dirname(dest_path)
    if dest_parent:
        os.makedirs(dest_parent, exist_ok=True)

    try:
        shutil.copy2(src_path, dest_path)
        return dest_path
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ copy file output v√†o GRADIO_OUTPUT_DIR: {e}")
        return None

def load_model(backbone_choice: str, codec_choice: str, device_choice: str, 
               force_lmdeploy: bool, custom_model_id: str = "", custom_base_model: str = "", 
               custom_hf_token: str = ""):
    """Load model with optimizations and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    lmdeploy_error_reason = None
    model_loaded = False # Ensure we don't try to use a half-loaded model
    
    yield (
        "‚è≥ ƒêang t·∫£i model v·ªõi t·ªëi ∆∞u h√≥a... L∆∞u √Ω: Qu√° tr√¨nh n√†y s·∫Ω t·ªën th·ªùi gian. Vui l√≤ng ki√™n nh·∫´n.",
        gr.update(interactive=False),
        gr.update(interactive=False),
        gr.update(interactive=False),
        gr.update(),
        gr.update(), gr.update(), gr.update(), gr.update()
    )
    
    try:
        # Cleanup before loading new model
        if tts is not None:
            tts = None # Reset instead of del to avoid NameError if load fails
            cleanup_gpu_memory()
        
        # Prepare Backbone Config/Repo
        custom_loading = False
        is_merged_lora = False

        if backbone_choice == "Custom Model":
            custom_loading = True
            if not custom_model_id or not custom_model_id.strip():
                yield (
                    "‚ùå L·ªói: Vui l√≤ng nh·∫≠p Model ID cho Custom Model.",
                    gr.update(interactive=False), gr.update(interactive=True), gr.update(interactive=False), gr.update(),
                    gr.update(), gr.update(), gr.update(), gr.update()
                )
                return

            # Check if it is a LoRA to merge
            if "lora" in custom_model_id.lower():
                # Merging mode
                print(f"üîÑ Detected LoRA in name. preparing merge with base: {custom_base_model}")
                if custom_base_model not in BACKBONE_CONFIGS:
                    yield (
                        f"‚ùå L·ªói: Base Model '{custom_base_model}' kh√¥ng h·ª£p l·ªá.",
                        gr.update(interactive=False), gr.update(interactive=True), gr.update(interactive=False),
                        gr.update(), gr.update(), gr.update(), gr.update(), gr.update()
                    )
                    return
                
                base_config = BACKBONE_CONFIGS[custom_base_model]
                backbone_config = {
                    "repo": base_config["repo"], # Load base first
                    "supports_streaming": base_config["supports_streaming"],
                    "description": f"Custom Merged: {custom_model_id} + {custom_base_model}"
                }
                is_merged_lora = True
            else:
                # Normal custom model
                backbone_config = {
                    "repo": custom_model_id.strip(),
                    "supports_streaming": False, # Assume false for unknown
                    "description": f"Custom Model: {custom_model_id}"
                }
        else:
            backbone_config = BACKBONE_CONFIGS[backbone_choice]
            
        codec_config = CODEC_CONFIGS[codec_choice]
        
        # Override LMDeploy if custom
        if custom_loading:
             if "gguf" in backbone_config['repo'].lower():
                 # GGUF must use Standard backend (llama-cpp)
                 use_lmdeploy = False
             elif is_merged_lora:
                 # LoRA can use LMDeploy if we merge first (checked logic below) or Standard
                 use_lmdeploy = force_lmdeploy and should_use_lmdeploy(custom_base_model, device_choice)
             else:
                 # Full custom model (e.g. finetune)
                 use_lmdeploy = force_lmdeploy and should_use_lmdeploy("VieNeu-TTS (GPU)", device_choice) # Assume GPU compatible?
        else:
             use_lmdeploy = force_lmdeploy and should_use_lmdeploy(backbone_choice, device_choice)
        
        if use_lmdeploy:
            lmdeploy_error_reason = None
            print(f"üöÄ Using LMDeploy backend with optimizations")
            
            backbone_device = "cuda"
            
            if "ONNX" in codec_choice:
                codec_device = "cpu"
            else:
                codec_device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # Special handling for Custom LoRA + LMDeploy -> Merge & Save
            target_backbone_repo = backbone_config["repo"]
            
            if custom_loading and is_merged_lora:
                safe_name = custom_model_id.strip().replace("/", "_").replace("\\", "_").replace(":", "")
                cache_dir = os.path.join("merged_models_cache", safe_name)
                target_backbone_repo = os.path.abspath(cache_dir)
                
                # Check if already merged (and voices.json exists)
                if not os.path.exists(cache_dir) or not os.path.exists(os.path.join(cache_dir, "vocab.json")):
                    print(f"üîÑ Merging LoRA for LMDeploy optimization: {cache_dir}")
                    if os.path.exists(cache_dir):
                        print("   ‚ö†Ô∏è Detected incomplete cache, rebuilding...")
                    yield (
                         f"‚è≥ ƒêang merge v√† l∆∞u model LoRA ƒë·ªÉ t·ªëi ∆∞u cho LMDeploy (thao t√°c n√†y ch·ªâ ch·∫°y m·ªôt l·∫ßn)...",
                         gr.update(interactive=False),
                         gr.update(interactive=False),
                         gr.update(interactive=False),
                         gr.update(),
                         gr.update(), gr.update(), gr.update(), gr.update()
                    )
                    
                    try:
                        # Use GPU for merging if available for speed
                        # We use the Base Model specified
                        base_repo = BACKBONE_CONFIGS[custom_base_model]["repo"]
                        merge_device = "cuda" if torch.cuda.is_available() else "cpu"
                        
                        print(f"   ‚Ä¢ Loading base: {base_repo} ({merge_device})")
                        temp_tts = VieNeuTTS(
                            backbone_repo=base_repo,
                            backbone_device=merge_device, 
                            codec_repo=codec_config["repo"],
                            codec_device="cpu", # Codec unused for merging, keep on CPU
                            hf_token=custom_hf_token
                        )
                        
                        print(f"   ‚Ä¢ Loading Adapter: {custom_model_id}")
                        temp_tts.load_lora_adapter(custom_model_id.strip(), hf_token=custom_hf_token)
                        
                        print(f"   ‚Ä¢ Merging...")
                        if hasattr(temp_tts.backbone, "merge_and_unload"):
                            temp_tts.backbone = temp_tts.backbone.merge_and_unload()
                        
                        print(f"   ‚Ä¢ Saving to cache: {cache_dir}")
                        temp_tts.backbone.save_pretrained(cache_dir)
                        temp_tts.tokenizer.save_pretrained(cache_dir)
                        
                        # Fix for LMDeploy: Explicitly save legacy tokenizer files (vocab.json, merges.txt)
                        # because LMDeploy/Transformers might default to slow tokenizer if fast one has issues,
                        # and save_pretrained on fast tokenizer sometimes omits legacy files.
                        try:
                            print("   ‚Ä¢ Ensuring legacy tokenizer files...")
                            from transformers import AutoTokenizer
                            slow_tokenizer = AutoTokenizer.from_pretrained(base_repo, use_fast=False)
                            slow_tokenizer.save_pretrained(cache_dir)
                        except Exception as e:
                            print(f"   ‚ö†Ô∏è Warning: Could not save slow tokenizer files: {e}")

                        # Save voices.json to cache directory so FastVieNeuTTS can find it
                        print(f"   ‚Ä¢ Saving voices definition...")
                        import json
                        voices_json_path = os.path.join(cache_dir, "voices.json")
                        voices_content = {
                             "meta": { "note": "Automatically generated during LoRA merge" },
                             "default_voice": temp_tts._default_voice,
                             "presets": temp_tts._preset_voices
                        }
                        with open(voices_json_path, 'w', encoding='utf-8') as f:
                             json.dump(voices_content, f, ensure_ascii=False, indent=2)

                        del temp_tts
                        cleanup_gpu_memory()
                        print("   ‚úÖ Merge & Save successfully!")
                        
                    except Exception as e:
                        import traceback
                        traceback.print_exc()
                        raise RuntimeError(f"Failed to merge & save LoRA for LMDeploy: {e}")

            print(f"üì¶ Loading optimized model...")
            print(f"   Backbone: {target_backbone_repo} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            print(f"   Triton: Enabled")
            
            try:
                tts = FastVieNeuTTS(
                    backbone_repo=target_backbone_repo,
                    backbone_device=backbone_device,
                    codec_repo=codec_config["repo"],
                    codec_device=codec_device,
                    memory_util=0.3,
                    tp=1,
                    enable_prefix_caching=True,
                    enable_triton=True,
                    hf_token=custom_hf_token
                )
                using_lmdeploy = True
                
                # Legacy caching removed
                print(f"   ‚úÖ Optimized backend initialized")
                
            except Exception as e:
                import traceback
                traceback.print_exc()
                
                error_str = str(e)
                if "$env:CUDA_PATH" in error_str:
                    lmdeploy_error_reason = "Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CUDA_PATH. Vui l√≤ng c√†i ƒë·∫∑t NVIDIA GPU Computing Toolkit."
                else:
                    lmdeploy_error_reason = f"{error_str}"
                
                yield (
                    f"‚ö†Ô∏è LMDeploy Init Error: {lmdeploy_error_reason}. ƒêang loading model v·ªõi backend m·∫∑c ƒë·ªãnh - t·ªëc ƒë·ªô ch·∫≠m h∆°n so v·ªõi lmdeploy...",
                    gr.update(interactive=False),
                    gr.update(interactive=False),
                    gr.update(interactive=False),
                    gr.update(),
                    gr.update(), gr.update(), gr.update(), gr.update()
                )
                time.sleep(1)
                use_lmdeploy = False
                using_lmdeploy = False
        
        if not use_lmdeploy:
            print(f"üì¶ Using original backend")

            if device_choice == "Auto":
                if "gguf" in backbone_config['repo'].lower():
                    # GGUF: uses Metal on Mac, CUDA on Windows/Linux
                    if sys.platform == "darwin":
                        backbone_device = "gpu"  # llama-cpp-python uses Metal
                    else:
                        backbone_device = "gpu" if torch.cuda.is_available() else "cpu"
                else:
                    # PyTorch model
                    if sys.platform == "darwin":
                        backbone_device = "mps" if torch.backends.mps.is_available() else "cpu"
                    else:
                        backbone_device = "cuda" if torch.cuda.is_available() else "cpu"

                # Codec device
                if "ONNX" in codec_choice:
                    codec_device = "cpu"
                elif sys.platform == "darwin":
                    codec_device = "mps" if torch.backends.mps.is_available() else "cpu"
                else:
                    codec_device = "cuda" if torch.cuda.is_available() else "cpu"

            elif device_choice == "MPS":
                backbone_device = "mps"
                codec_device = "mps" if "ONNX" not in codec_choice else "cpu"

            else:
                backbone_device = device_choice.lower()
                codec_device = device_choice.lower()

                if "ONNX" in codec_choice:
                    codec_device = "cpu"

            if "gguf" in backbone_config['repo'].lower() and backbone_device == "cuda":
                backbone_device = "gpu"
            
            print(f"üì¶ Loading model...")
            print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            
            tts = VieNeuTTS(
                backbone_repo=backbone_config["repo"],
                backbone_device=backbone_device,
                codec_repo=codec_config["repo"],
                codec_device=codec_device,
                hf_token=custom_hf_token
            )

            # Perform LoRA Merge if needed (ONLY for Standard Backend)
            # For LMDeploy, we handled it above by saving to disk
            if is_merged_lora and custom_loading and not using_lmdeploy:
                yield (
                    f"üîÑ ƒêang t·∫£i v√† merge LoRA adapter: {custom_model_id}...",
                    gr.update(interactive=False), gr.update(interactive=False), gr.update(interactive=False), gr.update(),
                    gr.update(), gr.update(), gr.update(), gr.update()
                )
                try:
                    # 1. Load Adapter
                    tts.load_lora_adapter(custom_model_id.strip(), hf_token=custom_hf_token)
                    
                    # 2. Merge and Unload
                    # Check if backbone matches expected type for merge
                    if hasattr(tts, 'backbone') and hasattr(tts.backbone, 'merge_and_unload'):
                        print("   üîÑ Merging LoRA into backbone...")
                        tts.backbone = tts.backbone.merge_and_unload()
                        
                        # Reset LoRA state so it behaves like a normal model
                        tts._lora_loaded = False 
                        tts._current_lora_repo = None
                        print("   ‚úÖ Merged successfully!")
                    else:
                        print("   ‚ö†Ô∏è Warning: Model does not support merge_and_unload, keeping adapter active.")
                        
                except Exception as e:
                     raise RuntimeError(f"Failed to merge LoRA: {e}")

            using_lmdeploy = False
        
        current_backbone = backbone_choice
        current_codec = codec_choice
        model_loaded = True
        
        # Success message with optimization info
        backend_name = "üöÄ LMDeploy (Optimized)" if using_lmdeploy else "üì¶ Standard"
        device_info = "cuda" if use_lmdeploy else (backbone_device if not use_lmdeploy else "N/A")
        
        streaming_support = "‚úÖ C√≥" if backbone_config['supports_streaming'] else "‚ùå Kh√¥ng"
        preencoded_note = "\n‚ö†Ô∏è Codec n√†y c·∫ßn s·ª≠ d·ª•ng pre-encoded codes (.pt files)" if codec_config['use_preencoded'] else ""
        
        opt_info = ""
        if using_lmdeploy and hasattr(tts, 'get_optimization_stats'):
            stats = tts.get_optimization_stats()
            opt_info = (
                f"\n\nüîß T·ªëi ∆∞u h√≥a:"
                f"\n  ‚Ä¢ Triton: {'‚úÖ' if stats['triton_enabled'] else '‚ùå'}"
                f"\n  ‚Ä¢ Max Batch Size (Default): {stats.get('max_batch_size', 'N/A')}"
                f"\n  ‚Ä¢ Reference Cache: {stats['cached_references']} voices"
                f"\n  ‚Ä¢ Prefix Caching: ‚úÖ"
            )
        
        warning_msg = ""
        if lmdeploy_error_reason:
             warning_msg = (
                 f"\n\n‚ö†Ô∏è **C·∫£nh b√°o:** Kh√¥ng th·ªÉ k√≠ch ho·∫°t LMDeploy (Optimized Backend) do l·ªói sau:\n"
                 f"üëâ {lmdeploy_error_reason}\n"
                 f"üí° H·ªá th·ªëng ƒë√£ t·ª± ƒë·ªông chuy·ªÉn v·ªÅ ch·∫ø ƒë·ªô Standard (ch·∫≠m h∆°n)."
             )

        success_msg = get_model_status_message()
        if warning_msg:
            success_msg += warning_msg
            
        # Prepare voice update
        try:
            # Get voices with descriptions for UI from SDK
            voices = tts.list_preset_voices()
        except Exception:
            voices = []

        has_voices = len(voices) > 0
        
        if has_voices:
            default_v = tts._default_voice
            
            # Helper to get values list
            is_tuple = (len(voices) > 0 and isinstance(voices[0], tuple))
            voice_values = [v[1] for v in voices] if is_tuple else voices
            
            if not default_v and voice_values:
                 default_v = voice_values[0]

            # Ensure default_v is in the list and selected correctly
            if default_v and default_v not in voice_values:
                if is_tuple:
                    # Try to find a nice description if possible, else use ID
                    voices.append((default_v, default_v))
                else:
                    voices.append(default_v)
            
            # Sort voices by name/label for better UX
            if is_tuple:
                voices.sort(key=lambda x: str(x[0]))
            else:
                voices.sort()

            voice_update = gr.update(choices=voices, value=default_v, interactive=True)
            
            # Show Standard Tabs
            tab_p = gr.update(visible=True)
            tab_c = gr.update(visible=True)
            tab_sel = gr.update(selected="preset_mode")
            mode_state = "preset_mode"
        else:
            # Missing voices.json case
            msg = "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file voices.json. Vui l√≤ng d√πng Tab Voice Cloning."
            voice_update = gr.update(choices=[msg], value=msg, interactive=False)
            
            # Show Preset Tab (to see message) and Custom Tab
            tab_p = gr.update(visible=True)
            tab_c = gr.update(visible=True)
            tab_sel = gr.update(selected="preset_mode")
            mode_state = "preset_mode"

        yield (
            success_msg,
            gr.update(interactive=True), # btn_generate
            gr.update(interactive=True), # btn_load
            gr.update(interactive=False), # btn_stop
            voice_update,
            tab_p, tab_c, tab_sel, mode_state
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        model_loaded = False
        using_lmdeploy = False

        if "$env:CUDA_PATH" in str(e):
            yield (
                "‚ùå L·ªói khi t·∫£i model: Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CUDA_PATH. Vui l√≤ng c√†i ƒë·∫∑t NVIDIA GPU Computing Toolkit (https://developer.nvidia.com/cuda/toolkit)",
                gr.update(interactive=False),
                gr.update(interactive=True),
                gr.update(interactive=False),
                gr.update(),
                gr.update(), gr.update(), gr.update(), gr.update()
            )
        else: 
            yield (
                f"‚ùå L·ªói khi t·∫£i model: {str(e)}",
                gr.update(interactive=False),
                gr.update(interactive=True),
                gr.update(interactive=False),
                gr.update(),
                gr.update(), gr.update(), gr.update(), gr.update()
            )


# --- 2. DATA & HELPERS ---

def _parse_terminology(text: str) -> dict:
    """Parse terminology text (t·ª´=phoneme per line) into dict. Runtime only."""
    if not text or not text.strip():
        return {}
    result = {}
    for line in text.strip().splitlines():
        line = line.strip()
        if not line or line.startswith('#'):
            continue
        if '=' in line:
            word, phoneme = line.split('=', 1)
        elif '\t' in line:
            word, phoneme = line.split('\t', 1)
        else:
            continue
        word = word.strip()
        phoneme = phoneme.strip()
        if word:
            result[word] = phoneme
    return result


def synthesize_speech(text: str, voice_choice: str, custom_audio, custom_text: str, 
                      mode_tab: str, generation_mode: str, use_batch: bool, max_batch_size_run: int,
                      temperature: float, max_chars_chunk: int, output_file_name: Optional[str] = None,
                      terminology_text: str = ""):
    """Synthesis with optimization support and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    
    if not model_loaded or tts is None:
        yield None, "‚ö†Ô∏è Vui l√≤ng t·∫£i model tr∆∞·ªõc!"
        return
    
    if not text or text.strip() == "":
        yield None, "‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n!"
        return
    
    raw_text = text.strip()
    phoneme_dict = _parse_terminology(terminology_text) if terminology_text else None
    
    codec_config = CODEC_CONFIGS[current_codec]
    use_preencoded = codec_config['use_preencoded']
    
    
    # Setup Reference
    yield None, "üìÑ ƒêang x·ª≠ l√Ω Reference..."
    
    try:
        ref_codes = None
        ref_text_raw = ""
        
        if mode_tab == "preset_mode":
            if not voice_choice:
                raise ValueError("Vui l√≤ng ch·ªçn gi·ªçng m·∫´u.")
            if "‚ö†Ô∏è" in voice_choice:
                raise ValueError("Kh√¥ng c√≥ gi·ªçng m·∫´u kh·∫£ d·ª•ng. Vui l√≤ng chuy·ªÉn sang Tab Voice Cloning.")
            
            # Use SDK method - handles caching and JSON internally
            voice_data = tts.get_preset_voice(voice_choice)
            ref_codes = voice_data['codes']
            ref_text_raw = voice_data['text']
            
        elif mode_tab == "custom_mode":
            # Reference from Custom Cloning UI
            if custom_audio is None:
                 raise ValueError("Vui l√≤ng upload file Audio m·∫´u (Reference Audio)!")
            if not custom_text or not custom_text.strip():
                 raise ValueError("Vui l√≤ng nh·∫≠p n·ªôi dung vƒÉn b·∫£n c·ªßa Audio m·∫´u (Reference Text)!")
            
            ref_text_raw = custom_text.strip()
            ref_codes = tts.encode_reference(custom_audio)
            
        else:
            raise ValueError(f"Unknown mode: {mode_tab}")

        # Ensure numpy for inference
        if isinstance(ref_codes, torch.Tensor):
            ref_codes = ref_codes.cpu().numpy()

    except Exception as e:
        yield None, f"‚ùå L·ªói x·ª≠ l√Ω Reference Audio: {str(e)}"
        return
    
    text_chunks = split_text_into_chunks(raw_text, max_chars=max_chars_chunk)
    total_chunks = len(text_chunks)
    
    # === STANDARD MODE ===
    if generation_mode == "Standard (M·ªôt l·∫ßn)":
        backend_name = "LMDeploy" if using_lmdeploy else "Standard"
        batch_info = " (Batch Mode)" if use_batch and using_lmdeploy and total_chunks > 1 else ""
        
        # Show batch size info
        batch_size_info = ""
        if use_batch and using_lmdeploy and hasattr(tts, 'max_batch_size'):
            batch_size_info = f" [Max batch: {tts.max_batch_size}]"
        
        yield None, f"üöÄ B·∫Øt ƒë·∫ßu t·ªïng h·ª£p {backend_name}{batch_info}{batch_size_info} ({total_chunks} ƒëo·∫°n)..."
        
        all_wavs = []
        sr = 24000
        
        start_time = time.time()
        
        try:
            # Use batch processing if enabled and using LMDeploy
            if use_batch and using_lmdeploy and hasattr(tts, 'infer_batch') and total_chunks > 1:
                # Show how many mini-batches will be processed
                num_batches = (total_chunks + max_batch_size_run - 1) // max_batch_size_run
                
                yield None, f"‚ö° X·ª≠ l√Ω {num_batches} mini-batch(es) (max {max_batch_size_run} ƒëo·∫°n/batch)..."
                
                chunk_wavs = tts.infer_batch(
                    text_chunks, 
                    ref_codes=ref_codes, 
                    ref_text=ref_text_raw,
                    max_batch_size=max_batch_size_run,
                    temperature=temperature,
                    phoneme_dict=phoneme_dict
                )
                
                for chunk_wav in chunk_wavs:
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_wavs.append(chunk_wav)

            else:
                # Sequential processing
                for i, chunk in enumerate(text_chunks):
                    yield None, f"‚è≥ ƒêang x·ª≠ l√Ω ƒëo·∫°n {i+1}/{total_chunks}..."
                    
                    chunk_wav = tts.infer(
                        chunk, 
                        ref_codes=ref_codes, 
                        ref_text=ref_text_raw,
                        temperature=temperature,
                        phoneme_dict=phoneme_dict
                    )
                    
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_wavs.append(chunk_wav)
            
            if not all_wavs:
                yield None, "‚ùå Kh√¥ng sinh ƒë∆∞·ª£c audio n√†o."
                return
            
            yield None, "üíæ ƒêang gh√©p file v√† l∆∞u..."
            
            # Use utility function for joining with silence/crossfade
            # Default silence=0.15s to match SDK
            final_wav = join_audio_chunks(all_wavs, sr=sr, silence_p=0.15)
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                output_path = tmp.name

            copy_output_if_configured(output_path, output_file_name=output_file_name)
            
            process_time = time.time() - start_time
            backend_info = f" (Backend: {'LMDeploy üöÄ' if using_lmdeploy else 'Standard üì¶'})"
            speed_info = f", T·ªëc ƒë·ªô: {len(final_wav)/sr/process_time:.2f}x realtime" if process_time > 0 else ""
            
            
            yield output_path, f"‚úÖ Ho√†n t·∫•t! (Th·ªùi gian: {process_time:.2f}s{speed_info}){backend_info}"
            
            # Cleanup memory
            if using_lmdeploy and hasattr(tts, 'cleanup_memory'):
                tts.cleanup_memory()
            
            cleanup_gpu_memory()
            
        except torch.cuda.OutOfMemoryError as e:
            cleanup_gpu_memory()
            yield None, (
                f"‚ùå GPU h·∫øt VRAM! H√£y th·ª≠:\n"
                f"‚Ä¢ Gi·∫£m Max Batch Size (hi·ªán t·∫°i: {tts.max_batch_size if hasattr(tts, 'max_batch_size') else 'N/A'})\n"
                f"‚Ä¢ Gi·∫£m ƒë·ªô d√†i vƒÉn b·∫£n\n\n"
                f"Chi ti·∫øt: {str(e)}"
            )
            return
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            cleanup_gpu_memory()
            yield None, f"‚ùå L·ªói Standard Mode: {str(e)}"
            return
    
    # === STREAMING MODE ===
    else:
        sr = 24000
        crossfade_samples = int(sr * 0.03)
        audio_queue = queue.Queue(maxsize=100)
        PRE_BUFFER_SIZE = 3
        
        end_event = threading.Event()
        error_event = threading.Event()
        error_msg = ""
        
        def producer_thread():
            nonlocal error_msg
            try:
                previous_tail = None
                
                for i, chunk_text in enumerate(text_chunks):
                    stream_gen = tts.infer_stream(
                        chunk_text, 
                        ref_codes=ref_codes, 
                        ref_text=ref_text_raw,
                        temperature=temperature,
                        phoneme_dict=phoneme_dict
                    )
                    
                    for part_idx, audio_part in enumerate(stream_gen):
                        if audio_part is None or len(audio_part) == 0:
                            continue
                        
                        if previous_tail is not None and len(previous_tail) > 0:
                            overlap = min(len(previous_tail), len(audio_part), crossfade_samples)
                            if overlap > 0:
                                fade_out = np.linspace(1.0, 0.0, overlap, dtype=np.float32)
                                fade_in = np.linspace(0.0, 1.0, overlap, dtype=np.float32)
                                
                                blended = (audio_part[:overlap] * fade_in + 
                                         previous_tail[-overlap:] * fade_out)
                                
                                processed = np.concatenate([
                                    previous_tail[:-overlap] if len(previous_tail) > overlap else np.array([]),
                                    blended,
                                    audio_part[overlap:]
                                ])
                            else:
                                processed = np.concatenate([previous_tail, audio_part])
                            
                            tail_size = min(crossfade_samples, len(processed))
                            previous_tail = processed[-tail_size:].copy()
                            output_chunk = processed[:-tail_size] if len(processed) > tail_size else processed
                        else:
                            tail_size = min(crossfade_samples, len(audio_part))
                            previous_tail = audio_part[-tail_size:].copy()
                            output_chunk = audio_part[:-tail_size] if len(audio_part) > tail_size else audio_part
                        
                        if len(output_chunk) > 0:
                            audio_queue.put((sr, output_chunk))
                
                if previous_tail is not None and len(previous_tail) > 0:
                    audio_queue.put((sr, previous_tail))
                    
            except Exception as e:
                import traceback
                traceback.print_exc()
                error_msg = str(e)
                error_event.set()
            finally:
                end_event.set()
                audio_queue.put(None)
        
        threading.Thread(target=producer_thread, daemon=True).start()
        
        yield (sr, np.zeros(int(sr * 0.05))), "üìÑ ƒêang buffering..."
        
        pre_buffer = []
        while len(pre_buffer) < PRE_BUFFER_SIZE:
            try:
                item = audio_queue.get(timeout=5.0)
                if item is None:
                    break
                pre_buffer.append(item)
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    return
                break
        
        full_audio_buffer = []
        backend_info = "üöÄ LMDeploy" if using_lmdeploy else "üì¶ Standard"
        for sr, audio_data in pre_buffer:
            full_audio_buffer.append(audio_data)
            yield (sr, audio_data), f"üîä ƒêang ph√°t ({backend_info})..."
        
        while True:
            try:
                item = audio_queue.get(timeout=0.05)
                if item is None:
                    break
                sr, audio_data = item
                full_audio_buffer.append(audio_data)
                yield (sr, audio_data), f"üîä ƒêang ph√°t ({backend_info})..."
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    break
                if end_event.is_set() and audio_queue.empty():
                    break
                continue
        
        if full_audio_buffer:
            final_wav = np.concatenate(full_audio_buffer)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                output_path = tmp.name

            copy_output_if_configured(output_path, output_file_name=output_file_name)

            yield output_path, f"‚úÖ Ho√†n t·∫•t Streaming! ({backend_info})"

            # Cleanup memory
            if using_lmdeploy and hasattr(tts, 'cleanup_memory'):
                tts.cleanup_memory()

            cleanup_gpu_memory()


# --- 4. UI SETUP ---
theme = gr.themes.Soft(
    primary_hue="indigo",
    secondary_hue="cyan",
    neutral_hue="slate",
    font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui'],
).set(
    button_primary_background_fill="linear-gradient(90deg, #6366f1 0%, #0ea5e9 100%)",
    button_primary_background_fill_hover="linear-gradient(90deg, #4f46e5 0%, #0284c7 100%)",
)

css = """
.container { max-width: 1400px; margin: auto; }
.header-box {
    text-align: center;
    margin-bottom: 25px;
    padding: 25px;
    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
    border-radius: 12px;
    color: white !important;
}
.header-title {
    font-size: 2.5rem;
    font-weight: 800;
    color: white !important;
}
.gradient-text {
    background: -webkit-linear-gradient(45deg, #60A5FA, #22D3EE);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}
.header-icon {
    color: white;
}
.status-box {
    font-weight: 500;
    border: 1px solid rgba(99, 102, 241, 0.1);
    background: rgba(99, 102, 241, 0.03);
    border-radius: 8px;
}
.status-box textarea {
    text-align: center;
    font-family: inherit;
}
.model-card-content {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    align-items: center;
    gap: 15px;
    font-size: 0.9rem;
    text-align: center;
    color: white !important;
}
.model-card-item {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 6px;
    color: white !important;
}
.model-card-item strong {
    color: white !important;
}
.model-card-item span {
    color: white !important;
}
.model-card-link {
    color: #60A5FA;
    text-decoration: none;
    font-weight: 500;
    transition: color 0.2s;
}
.model-card-link:hover {
    color: #22D3EE;
    text-decoration: underline;
}
.warning-banner {
    background-color: #fffbeb;
    border: 1px solid #fef3c7;
    border-radius: 12px;
    padding: 16px;
    margin-bottom: 20px;
}
.warning-banner-title {
    color: #92400e;
    font-weight: 700;
    font-size: 1.1rem;
    display: flex;
    align-items: center;
    gap: 8px;
    margin-bottom: 12px;
}
.warning-banner-grid {
    display: flex;
    gap: 15px;
    flex-wrap: wrap;
}
.warning-banner-item {
    flex: 1;
    min-width: 240px;
    background: #fef3c7;
    padding: 12px;
    border-radius: 8px;
    border: 1px solid #fde68a;
}
.warning-banner-item strong {
    color: #b45309;
    display: block;
    margin-bottom: 4px;
    font-size: 0.95rem;
}
.warning-banner-content {
    color: #78350f;
    font-size: 0.9rem;
    line-height: 1.5;
}
.warning-banner-content b {
    color: #451a03;
    background: rgba(251, 191, 36, 0.2);
    padding: 1px 4px;
    border-radius: 4px;
}
"""

EXAMPLES_LIST = [
    ["V·ªÅ mi·ªÅn T√¢y kh√¥ng ch·ªâ ƒë·ªÉ ng·∫Øm nh√¨n s√¥ng n∆∞·ªõc h·ªØu t√¨nh, m√† c√≤n ƒë·ªÉ c·∫£m nh·∫≠n t·∫•m ch√¢n t√¨nh c·ªßa ng∆∞·ªùi d√¢n n∆°i ƒë√¢y.", "Vƒ©nh (nam mi·ªÅn Nam)"],
    ["H√† N·ªôi nh·ªØng ng√†y v√†o thu mang m·ªôt v·∫ª ƒë·∫πp tr·∫ßm m·∫∑c v√† c·ªï k√≠nh ƒë·∫øn l·∫° th∆∞·ªùng.", "B√¨nh (nam mi·ªÅn B·∫Øc)"],
]


# Favicon (Parrot Emoji)
head_html = """
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶ú</text></svg>">
"""

with gr.Blocks(theme=theme, css=css, title="VieNeu-TTS", head=head_html) as demo:

    with gr.Column(elem_classes="container"):
        gr.HTML("""
<div class="header-box">
    <h1 class="header-title">
        <span class="header-icon">ü¶ú</span>
        <span class="gradient-text">VieNeu-TTS Studio</span>
    </h1>
    <div class="model-card-content">
        <div class="model-card-item">
            <strong>Models:</strong>
            <a href="https://huggingface.co/pnnbao-ump/VieNeu-TTS" target="_blank" class="model-card-link">VieNeu-TTS</a>
            <span>‚Ä¢</span>
            <a href="https://huggingface.co/pnnbao-ump/VieNeu-TTS-0.3B" target="_blank" class="model-card-link">VieNeu-TTS-0.3B</a>
        </div>
        <div class="model-card-item">
            <strong>Repository:</strong>
            <a href="https://github.com/pnnbao97/VieNeu-TTS" target="_blank" class="model-card-link">GitHub</a>
        </div>
        <div class="model-card-item">
            <strong>T√°c gi·∫£:</strong>
            <a href="https://www.facebook.com/bao.phamnguyenngoc.5" target="_blank" class="model-card-link">Ph·∫°m Nguy·ªÖn Ng·ªçc B·∫£o</a>
        </div>
        <div class="model-card-item">
            <strong>Discord:</strong>
            <a href="https://discord.gg/yJt8kzjzWZ" target="_blank" class="model-card-link">Tham gia c·ªông ƒë·ªìng</a>
        </div>
    </div>
</div>
        """)
        
        # --- CONFIGURATION ---
        with gr.Group():
            with gr.Row():
                backbone_select = gr.Dropdown(
                    list(BACKBONE_CONFIGS.keys()) + ["Custom Model"], 
                    value="VieNeu-TTS (GPU)", 
                    label="ü¶ú Backbone"
                )
                codec_select = gr.Dropdown(list(CODEC_CONFIGS.keys()), value="NeuCodec (Distill)", label="üéµ Codec")
                device_choice = gr.Radio(get_available_devices(), value="Auto", label="üñ•Ô∏è Device")
            
            with gr.Row(visible=False) as custom_model_group:
                custom_backbone_model_id = gr.Textbox(
                    label="üì¶ Custom Model ID",
                    placeholder="pnnbao-ump/VieNeu-TTS-0.3B-lora-ngoc-huyen",
                    info="Nh·∫≠p HuggingFace Repo ID ho·∫∑c ƒë∆∞·ªùng d·∫´n local",
                    scale=2
                )
                custom_backbone_hf_token = gr.Textbox(
                    label="üîë HF Token (n·∫øu private)",
                    placeholder="ƒê·ªÉ tr·ªëng n·∫øu repo public",
                    type="password",
                    info="Token ƒë·ªÉ truy c·∫≠p repo private",
                    scale=1
                )
                custom_backbone_base_model = gr.Dropdown(
                    [k for k in BACKBONE_CONFIGS.keys() if "gguf" not in k.lower()],
                    label="üîó Base Model (cho LoRA)",
                    value="VieNeu-TTS-0.3B (GPU)",
                    visible=False,
                    info="Model g·ªëc ƒë·ªÉ merge v·ªõi LoRA",
                    scale=1
                )
            
            with gr.Row():
                use_lmdeploy_cb = gr.Checkbox(
                    value=True, 
                    label="üöÄ Optimize with LMDeploy (Khuy√™n d√πng cho NVIDIA GPU)",
                    info="Tick n·∫øu b·∫°n d√πng GPU ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô t·ªïng h·ª£p ƒë√°ng k·ªÉ."
                )
            
            
            gr.Markdown("""
            üí° **S·ª≠ d·ª•ng Custom Model:** Ch·ªçn "Custom Model" ƒë·ªÉ t·∫£i LoRA adapter ho·∫∑c b·∫•t k·ª≥ model n√†o ƒë∆∞·ª£c finetune t·ª´ **VieNeu-TTS** ho·∫∑c **VieNeu-TTS-0.3B**.
            """)
            
            gr.HTML("""
            <div class="warning-banner">
                <div class="warning-banner-title">
                    ü¶ú G·ª£i √Ω t·ªëi ∆∞u hi·ªáu nƒÉng
                </div>
                <div class="warning-banner-grid">
                    <div class="warning-banner-item">
                        <strong>üê¢ H·ªá m√°y CPU</strong>
                        <div class="warning-banner-content">
                            S·ª≠ d·ª•ng <b>VieNeu-TTS-0.3B-q4-gguf</b> ƒë·ªÉ ƒë·∫°t t·ªëc ƒë·ªô x·ª≠ l√Ω nhanh nh·∫•t. N·∫øu ∆∞u ti√™n ƒë·ªô ch√≠nh x√°c th√¨ d√πng <b>VieNeu-TTS-0.3B-q8-gguf</b>.
                        </div>
                    </div>
                    <div class="warning-banner-item">
                        <strong>üêÜ H·ªá m√°y GPU</strong>
                        <div class="warning-banner-content">
                            Ch·ªçn <b>VieNeu-TTS-0.3B (GPU)</b> ƒë·ªÉ x2 t·ªëc ƒë·ªô (ƒë·ªô ch√≠nh x√°c ~95% b·∫£n g·ªëc).<br><br>
                            ‚ö†Ô∏è <b>L∆∞u √Ω GPU c≈© (RTX 10/20, T4):</b> C√°c GPU n√†y kh√¥ng h·ªó tr·ª£ bfloat16, n√™n khi d√πng LMDeploy <b>b·∫Øt bu·ªôc</b> ph·∫£i ch·ªçn <b>VieNeu-TTS-0.3B</b> thay v√¨ b·∫£n VieNeu-TTS g·ªëc.
                        </div>
                    </div>
                </div>
            </div>
            """)

            btn_load = gr.Button("üîÑ T·∫£i Model", variant="primary")
            model_status = gr.Markdown("‚è≥ Ch∆∞a t·∫£i model.")
        
        with gr.Row(elem_classes="container"):
            # --- INPUT ---
            with gr.Column(scale=3):
                text_input = gr.Textbox(
                    label=f"VƒÉn b·∫£n",
                    lines=4,
                    value="H√† N·ªôi, tr√°i tim c·ªßa Vi·ªát Nam, l√† m·ªôt th√†nh ph·ªë ng√†n nƒÉm vƒÉn hi·∫øn v·ªõi b·ªÅ d√†y l·ªãch s·ª≠ v√† vƒÉn h√≥a ƒë·ªôc ƒë√°o. B∆∞·ªõc ch√¢n tr√™n nh·ªØng con ph·ªë c·ªï k√≠nh quanh H·ªì Ho√†n Ki·∫øm, du kh√°ch nh∆∞ ƒë∆∞·ª£c du h√†nh ng∆∞·ª£c th·ªùi gian, chi√™m ng∆∞·ª°ng ki·∫øn tr√∫c Ph√°p c·ªï ƒëi·ªÉn h√≤a quy·ªán v·ªõi n√©t ki·∫øn tr√∫c truy·ªÅn th·ªëng Vi·ªát Nam. M·ªói con ph·ªë trong khu ph·ªë c·ªï mang m·ªôt t√™n g·ªçi ƒë·∫∑c tr∆∞ng, ph·∫£n √°nh ngh·ªÅ th·ªß c√¥ng truy·ªÅn th·ªëng t·ª´ng th·ªãnh h√†nh n∆°i ƒë√¢y nh∆∞ ph·ªë H√†ng B·∫°c, H√†ng ƒê√†o, H√†ng M√£. ·∫®m th·ª±c H√† N·ªôi c≈©ng l√† m·ªôt ƒëi·ªÉm nh·∫•n ƒë·∫∑c bi·ªát, t·ª´ t√¥ ph·ªü n√≥ng h·ªïi bu·ªïi s√°ng, b√∫n ch·∫£ th∆°m l·ª´ng tr∆∞a h√®, ƒë·∫øn ch√® Th√°i ng·ªçt ng√†o chi·ªÅu thu. Nh·ªØng m√≥n ƒÉn d√¢n d√£ n√†y ƒë√£ tr·ªü th√†nh bi·ªÉu t∆∞·ª£ng c·ªßa vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát, ƒë∆∞·ª£c c·∫£ th·∫ø gi·ªõi y√™u m·∫øn. Ng∆∞·ªùi H√† N·ªôi n·ªïi ti·∫øng v·ªõi t√≠nh c√°ch hi·ªÅn h√≤a, l·ªãch thi·ªáp nh∆∞ng c≈©ng r·∫•t c·∫ßu to√†n trong t·ª´ng chi ti·∫øt nh·ªè, t·ª´ c√°ch pha tr√† sen cho ƒë·∫øn c√°ch ch·ªçn hoa sen t√¢y ƒë·ªÉ th∆∞·ªüng tr√†.",
                )
                
                with gr.Tabs() as tabs:
                    with gr.TabItem("üë§ Preset", id="preset_mode") as tab_preset:
                        voice_select = gr.Dropdown(choices=[], value=None, label="Gi·ªçng m·∫´u")
                    
                    with gr.TabItem("ü¶ú Voice Cloning", id="custom_mode") as tab_custom:
                        custom_audio = gr.Audio(label="Audio gi·ªçng m·∫´u (3-5 gi√¢y) (.wav)", type="filepath")
                        custom_text = gr.Textbox(label="N·ªôi dung audio m·∫´u - vui l√≤ng g√µ ƒë√∫ng n·ªôi dung c·ªßa audio m·∫´u - k·ªÉ c·∫£ d·∫•u c√¢u v√¨ model r·∫•t nh·∫°y c·∫£m v·ªõi d·∫•u c√¢u (.,?!)")
                        gr.Examples(
                            examples=[
                                [os.path.join("examples", "audio_ref", "example.wav"), "V√≠ d·ª• 2. T√≠nh trung b√¨nh c·ªßa d√£y s·ªë."],
                                [os.path.join("examples", "audio_ref", "example_2.wav"), "Tr√™n th·ª±c t·∫ø, c√°c nghi ng·ªù ƒë√£ b·∫Øt ƒë·∫ßu xu·∫•t hi·ªán."],
                                [os.path.join("examples", "audio_ref", "example_3.wav"), "C·∫≠u c√≥ nh√¨n th·∫•y kh√¥ng?"],
                                [os.path.join("examples", "audio_ref", "example_4.wav"), "T·∫øt l√† d·ªãp m·ªçi ng∆∞·ªùi h√°o h·ª©c ƒë√≥n ch√†o m·ªôt nƒÉm m·ªõi v·ªõi nhi·ªÅu hy v·ªçng v√† mong ∆∞·ªõc."]
                            ],
                            inputs=[custom_audio, custom_text],
                            label="V√≠ d·ª• m·∫´u ƒë·ªÉ th·ª≠ nghi·ªám clone gi·ªçng"
                        )
                        
                        gr.Markdown("""
                        **üí° M·∫πo nh·ªè:** N·∫øu k·∫øt qu·∫£ Zero-shot Voice Cloning ch∆∞a nh∆∞ √Ω, b·∫°n h√£y c√¢n nh·∫Øc **Finetune (LoRA)** ƒë·ªÉ ƒë·∫°t ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t. 
                        H∆∞·ªõng d·∫´n chi ti·∫øt c√≥ t·∫°i file: `finetune/README.md` ho·∫∑c xem tr√™n [GitHub](https://github.com/pnnbao97/VieNeu-TTS/tree/main/finetune).
                        """)              
                
                generation_mode = gr.Radio(
                    ["Standard (M·ªôt l·∫ßn)"],
                    value="Standard (M·ªôt l·∫ßn)",
                    label="Ch·∫ø ƒë·ªô sinh"
                )
                with gr.Row():
                    use_batch = gr.Checkbox(
                        value=True, 
                        label="‚ö° Batch Processing",
                        info="X·ª≠ l√Ω nhi·ªÅu ƒëo·∫°n c√πng l√∫c (ch·ªâ √°p d·ª•ng khi s·ª≠ d·ª•ng GPU v√† ƒë√£ c√†i ƒë·∫∑t LMDeploy)"
                    )
                    max_batch_size_run = gr.Slider(
                        minimum=1, 
                        maximum=16, 
                        value=4, 
                        step=1, 
                        label="üìä Batch Size (Generation)",
                        info="S·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn b·∫£n x·ª≠ l√Ω c√πng l√∫c. Gi√° tr·ªã cao = nhanh h∆°n nh∆∞ng t·ªën VRAM h∆°n. Gi·∫£m xu·ªëng n·∫øu g·∫∑p l·ªói Out of Memory."
                    )
                
                with gr.Accordion("üìñ Thu·∫≠t ng·ªØ (Runtime)", open=False):
                    terminology_text = gr.Textbox(
                        label="ƒê·ªãnh nghƒ©a ph√°t √¢m t√πy ch·ªânh",
                        placeholder="M·ªói d√≤ng: t·ª´=phoneme (IPA)\nV√≠ d·ª•:\nAI=Ààe…™Ààa…™\nGPT= §ÀàiÀêpÀàiÀêtÀàiÀê",
                        lines=4,
                        info="Ch·ªâ √°p d·ª•ng trong phi√™n l√†m vi·ªác hi·ªán t·∫°i, kh√¥ng l∆∞u l·∫°i. D√πng = ho·∫∑c Tab ƒë·ªÉ ph√¢n t√°ch."
                    )
                
                with gr.Accordion("‚öôÔ∏è C√†i ƒë·∫∑t n√¢ng cao (Generation)", open=False):
                    with gr.Row():
                        temperature_slider = gr.Slider(
                            minimum=0.1, maximum=1.5, value=1.0, step=0.1,
                            label="üå°Ô∏è Temperature", 
                            info="ƒê·ªô s√°ng t·∫°o. Cao = ƒëa d·∫°ng c·∫£m x√∫c h∆°n nh∆∞ng d·ªÖ l·ªói. Th·∫•p = ·ªïn ƒë·ªãnh h∆°n."
                        )
                        max_chars_chunk_slider = gr.Slider(
                            minimum=128, maximum=512, value=256, step=32,
                            label="üìù Max Chars per Chunk",
                            info="ƒê·ªô d√†i t·ªëi ƒëa m·ªói ƒëo·∫°n x·ª≠ l√Ω."
                        )
                
                # State to track current mode (replaces unreliable Textbox/Tabs input)
                current_mode_state = gr.State("preset_mode")
                output_file_name_input = gr.Textbox(
                    label="Output File Name",
                    placeholder="V√≠ d·ª•: opm-tt/name.wav (ch·ªâ d√πng khi GRADIO_OUTPUT_DIR ƒë∆∞·ª£c set)",
                    info="Cho ph√©p subfolder; n·∫øu tr√πng t√™n s·∫Ω t·ª± th√™m h·∫≠u t·ªë th·ªùi gian.",
                )
                
                with gr.Row():
                    btn_generate = gr.Button("üéµ B·∫Øt ƒë·∫ßu", variant="primary", scale=2, interactive=False)
                    btn_stop = gr.Button("‚èπÔ∏è D·ª´ng", variant="stop", scale=1, interactive=False)
            
            # --- OUTPUT ---
            with gr.Column(scale=2):
                audio_output = gr.Audio(
                    label="K·∫øt qu·∫£",
                    type="filepath",
                    autoplay=True
                )
                status_output = gr.Textbox(
                    label="Tr·∫°ng th√°i", 
                    elem_classes="status-box",
                    lines=2,
                    max_lines=10,
                    show_copy_button=True
                )
                gr.Markdown("<div style='text-align: center; color: #64748b; font-size: 0.8rem;'>üîí Audio ƒë∆∞·ª£c ƒë√≥ng d·∫•u b·∫£n quy·ªÅn ·∫©n (Watermarker) ƒë·ªÉ b·∫£o m·∫≠t v√† ƒë·ªãnh danh AI.</div>")
        
        # # --- EVENT HANDLERS ---
        # def update_info(backbone: str) -> str:
        #     return f"Streaming: {'‚úÖ' if BACKBONE_CONFIGS[backbone]['supports_streaming'] else '‚ùå'}"
        
        # backbone_select.change(update_info, backbone_select, model_status)
        
        # Handler to show/hide Voice Cloning tab
        def on_codec_change(codec: str, current_mode: str):
            is_onnx = "onnx" in codec.lower()
            # If switching to ONNX and we are on custom mode, switch back to preset
            if is_onnx and current_mode == "custom_mode":
                return gr.update(visible=False), gr.update(selected="preset_mode"), "preset_mode"
            return gr.update(visible=not is_onnx), gr.update(), current_mode
        
        codec_select.change(
            on_codec_change, 
            inputs=[codec_select, current_mode_state], 
            outputs=[tab_custom, tabs, current_mode_state]
        )
        
        # Bind tab events to update state
        tab_preset.select(lambda: "preset_mode", outputs=current_mode_state)
        tab_custom.select(lambda: "custom_mode", outputs=current_mode_state)
        
        
        # --- Custom Model Event Handlers ---
        def on_backbone_change(choice):
            is_custom = (choice == "Custom Model")
            return gr.update(visible=is_custom)

        backbone_select.change(
            on_backbone_change,
            inputs=[backbone_select],
            outputs=[custom_model_group]
        )
        
        def on_custom_id_change(model_id):
            # Auto detect LoRA and base model
            if model_id and "lora" in model_id.lower():
                # Detect base model: if "0.3" in name -> 0.3B, else VieNeu-TTS
                if "0.3" in model_id:
                    base_model = "VieNeu-TTS-0.3B (GPU)"
                else:
                    base_model = "VieNeu-TTS (GPU)"
                
                return (
                    gr.update(visible=True, value=base_model),
                    gr.update(), gr.update()
                )
            
            return (
                gr.update(visible=False),
                gr.update(),
                gr.update()
            )
            
        custom_backbone_model_id.change(
            on_custom_id_change,
            inputs=[custom_backbone_model_id],
            outputs=[custom_backbone_base_model, custom_audio, custom_text]
        )

        btn_load.click(
            fn=load_model,
            inputs=[backbone_select, codec_select, device_choice, use_lmdeploy_cb,
                    custom_backbone_model_id, custom_backbone_base_model, custom_backbone_hf_token],
            outputs=[model_status, btn_generate, btn_load, btn_stop, voice_select, tab_preset, tab_custom, tabs, current_mode_state]
        )
        
        
        generate_event = btn_generate.click(
            fn=synthesize_speech,
            inputs=[text_input, voice_select, custom_audio, custom_text, current_mode_state, 
                    generation_mode, use_batch, max_batch_size_run,
                    temperature_slider, max_chars_chunk_slider, output_file_name_input,
                    terminology_text],
            outputs=[audio_output, status_output]
        )
        
        # When generation starts, enable stop button
        btn_generate.click(lambda: gr.update(interactive=True), outputs=btn_stop)
        # When generation ends/stops, disable stop button
        generate_event.then(lambda: gr.update(interactive=False), outputs=btn_stop)
        
        btn_stop.click(fn=None, cancels=[generate_event])
        btn_stop.click(lambda: (None, "‚èπÔ∏è ƒê√£ d·ª´ng t·∫°o gi·ªçng n√≥i."), outputs=[audio_output, status_output])
        btn_stop.click(lambda: gr.update(interactive=False), outputs=btn_stop)

        # Persistence: Restore UI state on load
        demo.load(
            fn=restore_ui_state,
            outputs=[model_status, btn_generate, btn_stop]
        )

if __name__ == "__main__":
    # Cho ph√©p override t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (h·ªØu √≠ch cho Docker)
    server_name = os.getenv("GRADIO_SERVER_NAME", "127.0.0.1")
    server_port = int(os.getenv("GRADIO_SERVER_PORT", "7860"))

    # Check running in Colab
    is_on_colab = os.getenv("COLAB_RELEASE_TAG") is not None

    # Default:
    # - Colab: share=True (convenient)
    # - Docker/local: share=False (safe)
    share = env_bool("GRADIO_SHARE", default=is_on_colab)
    #
    # If server_name is "0.0.0.0" and GRADIO_SHARE is not set, disable sharing
    if server_name == "0.0.0.0" and os.getenv("GRADIO_SHARE") is None:
        share = False

    demo.queue().launch(server_name=server_name, server_port=server_port, share=share)
